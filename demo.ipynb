{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demonstração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parser: para ler os dados de um arquivo\n",
    "from util.dataparser import DataParser as dp\n",
    "# Vizinhos mais próximos: força bruta\n",
    "from classifiers.neighbors import brute_force_k_neighbors \\\n",
    "    as vizinhos_forca_bruta\n",
    "# Classificador KNN\n",
    "from classifiers.neighbors import KNeighborsClassifier\n",
    "# Para realizar as medidas de impureza\n",
    "from metrics.measurescores import ImpurityMeasures\n",
    "# Biblioteca numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os dados do arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando o data parser e um arquivo de teste, podemos fazer a leitura dos dados e converter em um formato mais tradicional para trabalhar com algoritmos de aprendizado de máquina.\n",
    "A seguir, lemos um arquivo de teste no formato txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, X, y = dp.parse('datasets/test_data.txt')\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada linha da matriz X representa uma instância do data set e cada coluna representa um atributo e seus valores.\n",
    "A seguir, iremos ler o data set original da iris obtido pelo arquivo arff do weka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = None  # Não usaremos a coluna id\n",
    "X, y = dp.arff_data('datasets/iris.arff')\n",
    "\n",
    "print(X[:5])  # Imprimindo apenas as primeiras 5 instâncias\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizinhos mais próximos\n",
    "\n",
    "O algoritmo __k vizinhos mais próximos__ é um do algoritmos de aprendizado de máquina mais fáceis de implementar. Na verdade, ele é conhecido como um \"_lazy algorithm_\", ou algoritmo preguiçoso, principalmente pelo fato de não ser necessário uma fase de treinamento para predizer as classes de novas instancias.\n",
    "\n",
    "Em ```classifiers.neighbors```, implementamos um algoritmo de força bruta para encontrar os k vizinhos mais próximos.\n",
    "\n",
    "Para predizer a classe da instância, basta fazer uma votação das classes dos vizinhos mais próximos encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra os k vizinhos mais próximos da primeira instância, \n",
    "# neste caso, k = 5:\n",
    " \n",
    "distancias, vizinhos = vizinhos_forca_bruta(X, x=X[0], \n",
    "                                            n_neighbors=5)\n",
    "\n",
    "print(\"Vizinhos mais próximos={}\".format(vizinhos))\n",
    "print(\"Distâncias calculadas={}\".format(distancias))\n",
    "\n",
    "# Imprime as classes dos vizinhos encontrados\n",
    "for i in range(len(vizinhos)):\n",
    "    print(y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É facil perceber que a classe da primeira instância é a _Iris-setosa_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Nota:todas os métodos da biblioteca utilizam a função de força bruta acima em algum momento._\n",
    "\n",
    "Implementamos nosso classificador de forma análoga ao classificador da biblioteca sci-kit learn. Por isso, para classificar uma dada instância, podemos instanciar um objeto do KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instancia o classificador knn\n",
    "classifier = KNeighborsClassifier(algorithm='brute')  \n",
    "\n",
    "# Como a instância a ser predita está contida no data set de teste, \n",
    "# para não computar a própria instância no cálculo, podemos removê-la \n",
    "# do data set.\n",
    "\n",
    "# Transforma em lista:\n",
    "X = list(X)  \n",
    "y = list(y)\n",
    "# Remove o último elemento:\n",
    "instancia = X.pop()  \n",
    "classe = y.pop()\n",
    "\n",
    "classifier.fit(X, y)  # Executa o método fit\n",
    "\n",
    "# Prediz a instância x (o método predict aceita múltiplas instâncias, \n",
    "# por isso x está contido em um vetor):\n",
    "pred = classifier.predict([instancia])\n",
    "\n",
    "# Mostra os resultados:\n",
    "print(\"Predição={}\".format(pred))\n",
    "print(\"Real={}\".format(classe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizinhos mais próximos: algoritmo kd-tree\n",
    "\n",
    "Até agora vimos o algoritmo de classificação executando uma busca em força bruta para encontrar os vizinhos mais próximos, isso porque para cada instância a ser predita, seria calculado a distância com todas as instâncias do data set de teste, e depois selecionado as k instâncias com as menores distâncias.\n",
    "\n",
    "O sci-kit learn implementa um algoritmo extremamente eficiente para isso chamado kd-tree. A kd-tree é uma arvore onde as folhas agrupam elementos com valores de atributos parecidos, o que melhora muito o desempenho em buscas quando o data set é muito grande.\n",
    "\n",
    "Tentamos implementar o algoritmo kd-tree em nosso classificador. Apesar de não tão preciso quando o algoritmo do sci-kit learn, consegue predizer muitas instâncias corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(algorithm='kd_tree')\n",
    "\n",
    "classifier.fit(X, y)  # Executa o método fit\n",
    "\n",
    "instancia = X[0]\n",
    "\n",
    "# Prediz a instância x (o método predict aceita múltiplas instâncias, \n",
    "# por isso x está contido em um vetor):\n",
    "pred = classifier.predict([instancia], ignore_x=True)  \n",
    "\n",
    "# Mostra os resultados:\n",
    "print(\"Predição={}\".format(pred))\n",
    "print(\"Real={}\".format(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medidas de impureza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos interessados em medir as impurezas dos dados. Medir impurezas é extremamente útil, principalmente, por exemplo, se estivéssemos interessados em obter árvores de decisão para classificar novas instâncias.\n",
    "\n",
    "Implementamos as medidas gini, entropia e o erro de classificação na biblioteca measurescores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após instanciar um objeto de ```ImpurityMeasures``` com os nossos dados, podemos calcular as medidas de impureza de um dado atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instancia um objeto de ImpurityMeasures\n",
    "medidas = ImpurityMeasures(X, y)  \n",
    "\n",
    "# Iremos calcular para cada atributo(índice da coluna)\n",
    "for atributo in range(len(X[0])):  \n",
    "    print(\"Para o atributo {}\".format(atributo)) \n",
    "    print(\"Porcentagens={}\".format(medidas.percentages(atributo)[:10]))\n",
    "    print(\"Ginis={}\".format(medidas.gini(atributo)[:10]))\n",
    "    print(\"Entropias={}\".format(medidas.entropy(atributo)[:10]))\n",
    "    print(\"Erros de classificação={}\".format(medidas.\n",
    "                                             classification_error(atributo)[:10]))\n",
    "\n",
    "# Imprime a medida gini para o primeiro valor do primeiro atributo:\n",
    "print(\"Medida gini para o primeiro valor do primeiro atributo:\")\n",
    "print(medidas.gini(0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que para cada calculo de medida, todos os dados de impureza de cada instancia e cada valor de atributo são retornados (note que existem muitos valores repetidos, porque existem valores de atributos iguais para uma dada classe, isso implica que a impureza seja igual nesses casos).\n",
    "\n",
    "Talvez todos esses dados aparentem serem confusos, mas observe o retorno da função na documentação do método ```percentages``` por exemplo:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Compute all the percentages p(yi) of a given feature.\n",
    "\n",
    ":param feature: attribute index to compute the percentage\n",
    ":return: numpy array shape [attribute values, class labels]\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "O retorno da função é uma matriz onde cada linha representa o respectivo valor do atributo, e cada coluna a respectiva classe. Logo, o valor Pij da matriz representa a porcentagem P do valor do atributo na instancia i, com relação a classe j (note que a soma de cada linha é igual a 1!)\n",
    "\n",
    "Esse método é extremamente útil porque é utilizado para calcular todas as medidas de impureza.\n",
    "\n",
    "Já para a medida gini, a documentação fornece:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Compute the gini measures of a given feature in the data set.\n",
    "\n",
    ":param feature: feature to compute\n",
    ":return: numpy array shape [attribute values]\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Isto é, para cada valor de atributo de um dado atributo(coluna do data set), retornará um vetor contendo as medidas gini de cada valor (as demais medidas de impureza funcionam de forma análoga).\n",
    "\n",
    "Se quisessemos dividir o data set em faixas o mais puras possível, escolheríamos os valores de impureza menores possíveis e separaríamos as instâncias com valores menores e maiores que o escolhido.\n",
    "\n",
    "Observe para um data set menor os dados retornados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([[1, 1, 0],  # classe 1\n",
    "                [1, 0, 1],  # classe 2\n",
    "                [1, 0, 1],  # classe 2\n",
    "                [0, 1, 0],  # classe 1\n",
    "                [0, 1, 1]]) # classe 2 \n",
    "y = ['classe 1', 'classe 2', 'classe 2', 'classe 1', 'classe 2']\n",
    "\n",
    "medidas = ImpurityMeasures(X, y)\n",
    "\n",
    "# Iremos calcular para cada atributo(índice da coluna):\n",
    "for atributo in range(len(X[0])):  \n",
    "    print(\"Para o atributo {}\".format(atributo)) \n",
    "    print(\"Porcentagens={}\".format(medidas.percentages(atributo)))\n",
    "    print(\"Ginis={}\".format(medidas.gini(atributo)))\n",
    "    print(\"Entropias={}\".format(medidas.entropy(atributo)))\n",
    "    print(\"Erros de classificação={}\".format(medidas.\n",
    "                                             classification_error(atributo)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe os valores das medidas de impureza do último atributo, todos os valores do atributo dividem perfeitamente o data set nas duas classes que temos, isso implica a maior pureza possível, exatamente como obtido nos cálculos das medidas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
